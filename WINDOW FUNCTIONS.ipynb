{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57b49f3-3680-4f8b-8e82-f3b092e70954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+\n|  Customer Name|       Category|Sub Category|Sales|\n+---------------+---------------+------------+-----+\n|   Tom Prescott|      Furniture|      Chairs| 4000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|\n|    Joseph Holt|      Furniture|      Tables| 5000|\n|Alejandro Grove|      Furniture| Furnishings| 8000|\n|  Adrian Barton|Office Supplies|     Binders| 3000|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|\n|    Sean Miller|     Technology|    Machines|22000|\n|   Tamara Chand|     Technology|     Copiers|18000|\n|    John Murray|     Technology|      Phones| 5000|\n|Kelly Collister|     Technology| Accessories| 3000|\n+---------------+---------------+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sampleData = [\n",
    "('Tom Prescott', 'Furniture', 'Chairs', 4000),\n",
    "('Quincy Jones', 'Furniture', 'Bookcases', 4000),\n",
    "('Joseph Holt', 'Furniture', 'Tables', 5000),\n",
    "('Alejandro Grove', 'Furniture', 'Furnishings', 8000),\n",
    "('Adrian Barton', 'Office Supplies', 'Binders', 3000),\n",
    "('Ken Lonsdale', 'Office Supplies', 'Supplies', 9000),\n",
    "('Greg Guthrie', 'Office Supplies', 'Fasteners', 3000),\n",
    "('Yoseph Carroll', 'Office Supplies', 'Storage', 3000),\n",
    "('Sean Miller', 'Technology', 'Machines', 22000),\n",
    "('Tamara Chand', 'Technology', 'Copiers', 18000),\n",
    "('John Murray', 'Technology', 'Phones', 5000),\n",
    "('Kelly Collister', 'Technology', 'Accessories', 3000)\n",
    "]\n",
    "\n",
    "# column names for dataframe\n",
    "columns = [\"Customer Name\", \"Category\",\n",
    "           \"Sub Category\", \"Sales\"]\n",
    " \n",
    "# creating the dataframe df\n",
    "df = spark.createDataFrame(data=sampleData,\n",
    "                           schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31bf2cb-cc1c-4ce0-aa0d-e791cd1b946f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import * # this is needed for window functions\n",
    "from pyspark.sql.functions import * # this is needed for window functions\n",
    "from pyspark.sql.types import  *  # thi shas to be used when you are trying to assign datatypes and struct fields and struct types\n",
    "from pyspark.sql.column import * # this has to beused when you are trying to use the column functio in datafram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4aafc4-f658-4290-ae8d-fd29afbc2a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " to create a windo function in pyspark. there are 2 steps\n",
    "step-1: assign a partition and order level, either table or specific column partiton level\n",
    "step-2: write any function based on above partitiuon created\n",
    "\n",
    "for SQL table level : over(order by sales desc)\n",
    "pyspark code for table level: window.orderby(col(\"sales\").desc())\n",
    "\n",
    "for SQL partition level: over(partiton by category order by sales desc )\n",
    "pyspark code for partition level: windows.partitionBy(\"category).orderBy(col(\"sales).desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f5a5cf-b3b1-470b-81ae-8874a0898966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+----------+\n|  Customer Name|       Category|Sub Category|Sales|row_number|\n+---------------+---------------+------------+-----+----------+\n|    Sean Miller|     Technology|    Machines|22000|         1|\n|   Tamara Chand|     Technology|     Copiers|18000|         2|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|         3|\n|Alejandro Grove|      Furniture| Furnishings| 8000|         4|\n|    Joseph Holt|      Furniture|      Tables| 5000|         5|\n|    John Murray|     Technology|      Phones| 5000|         6|\n|   Tom Prescott|      Furniture|      Chairs| 4000|         7|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|         8|\n|  Adrian Barton|Office Supplies|     Binders| 3000|         9|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|        10|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|        11|\n|Kelly Collister|     Technology| Accessories| 3000|        12|\n+---------------+---------------+------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#create a row_number() for above table based on sales descending\n",
    "\n",
    "#step1: creatre a window partiton at table level\n",
    "from pyspark.sql.window import Window\n",
    "#step1: create a window partition at table level\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Corrected line:\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2: Create a row_number functgion\n",
    "df_row = df.withColumn(\"row_number\", row_number().over(table_partition))\n",
    "df_row.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c4ca27-5f31-4d8c-a5ea-f07dfe315a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+----------+----+----------+\n|  Customer Name|       Category|Sub Category|Sales|row_number|Rank|dense_rank|\n+---------------+---------------+------------+-----+----------+----+----------+\n|Alejandro Grove|      Furniture| Furnishings| 8000|         1|   1|         1|\n|    Joseph Holt|      Furniture|      Tables| 5000|         2|   2|         2|\n|   Tom Prescott|      Furniture|      Chairs| 4000|         3|   3|         3|\n|   Quincy Jones|      Furniture|   Bookcases| 4000|         4|   3|         3|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|         1|   1|         1|\n|  Adrian Barton|Office Supplies|     Binders| 3000|         2|   2|         2|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000|         3|   2|         2|\n| Yoseph Carroll|Office Supplies|     Storage| 3000|         4|   2|         2|\n|    Sean Miller|     Technology|    Machines|22000|         1|   1|         1|\n|   Tamara Chand|     Technology|     Copiers|18000|         2|   2|         2|\n|    John Murray|     Technology|      Phones| 5000|         3|   3|         3|\n|Kelly Collister|     Technology| Accessories| 3000|         4|   4|         4|\n+---------------+---------------+------------+-----+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# create a row_number() for abov table based on category partition and sales descending\n",
    "\n",
    "# step-1: Create a window partion at tabel level\n",
    "window_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step-2: create a row_number function\n",
    "df2 = df.withColumn(\"row_number\", row_number().over(window_cat))\\\n",
    "         .withColumn(\"Rank\",rank().over(window_cat))\\\n",
    "         .withColumn(\"dense_rank\",dense_rank().over(window_cat))  \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22260d36-8928-4980-8fcc-2cc7933937a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-----+-----+\n|  Customer Name|       Category|Sub Category|Sales| lag1| lag2|\n+---------------+---------------+------------+-----+-----+-----+\n|    Sean Miller|     Technology|    Machines|22000| NULL| NULL|\n|   Tamara Chand|     Technology|     Copiers|18000|22000| NULL|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|18000|22000|\n|Alejandro Grove|      Furniture| Furnishings| 8000| 9000|18000|\n|    Joseph Holt|      Furniture|      Tables| 5000| 8000| 9000|\n|    John Murray|     Technology|      Phones| 5000| 5000| 8000|\n|   Tom Prescott|      Furniture|      Chairs| 4000| 5000| 5000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000| 4000| 5000|\n|  Adrian Barton|Office Supplies|     Binders| 3000| 4000| 4000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000| 3000| 4000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000| 3000| 3000|\n|Kelly Collister|     Technology| Accessories| 3000| 3000| 3000|\n+---------------+---------------+------------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# LEAD : get the next row value into current row\n",
    "# syntax: lead(\"Colnmae\",<no_of_nextcolumn>).over(<partition level>)\n",
    "\n",
    "#Lag : get the last row value into current row\n",
    "# syntax: lag(\"Colnmae\",<no_of_previouscolumn>).over(<partition level>)\n",
    "\n",
    "#get the previous row sales into current row\n",
    "\n",
    "#step1 : create a window partition at table level\n",
    "\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "# step2: Create a laf function\n",
    "\n",
    "df_lag = df.withColumn(\"lag1\",lag(\"Sales\",1).over(table_partition))\\\n",
    "           .withColumn(\"lag2\",lag(\"Sales\",2).over(table_partition))\n",
    "           \n",
    "df_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487a4464-5642-4b40-8917-05759ef4b027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+-----+-----+\n|  Customer Name|       Category|Sub Category|Sales|lead1|lead2|\n+---------------+---------------+------------+-----+-----+-----+\n|    Sean Miller|     Technology|    Machines|22000|18000| 9000|\n|   Tamara Chand|     Technology|     Copiers|18000| 9000| 8000|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000| 8000| 5000|\n|Alejandro Grove|      Furniture| Furnishings| 8000| 5000| 5000|\n|    Joseph Holt|      Furniture|      Tables| 5000| 5000| 4000|\n|    John Murray|     Technology|      Phones| 5000| 4000| 4000|\n|   Tom Prescott|      Furniture|      Chairs| 4000| 4000| 3000|\n|   Quincy Jones|      Furniture|   Bookcases| 4000| 3000| 3000|\n|  Adrian Barton|Office Supplies|     Binders| 3000| 3000| 3000|\n|   Greg Guthrie|Office Supplies|   Fasteners| 3000| 3000| 3000|\n| Yoseph Carroll|Office Supplies|     Storage| 3000| 3000| NULL|\n|Kelly Collister|     Technology| Accessories| 3000| NULL| NULL|\n+---------------+---------------+------------+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Get the Next row sales into current row\n",
    "\n",
    "# Step1 : create a window partition at table level\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2: Create a lead function\n",
    "df_lead = df.withColumn(\"lead1\",lead(\"Sales\",1).over(table_partition))\\\n",
    "            .withColumn(\"lead2\",lead(\"Sales\",2).over(table_partition))\n",
    "df_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0602c532-acc4-4397-9dbe-9564317487f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+----------+\n|  Customer Name|       Category|Sub Category|Sales|row_number|\n+---------------+---------------+------------+-----+----------+\n|Alejandro Grove|      Furniture| Furnishings| 8000|         1|\n|    Joseph Holt|      Furniture|      Tables| 5000|         2|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|         1|\n|  Adrian Barton|Office Supplies|     Binders| 3000|         2|\n|    Sean Miller|     Technology|    Machines|22000|         1|\n|   Tamara Chand|     Technology|     Copiers|18000|         2|\n+---------------+---------------+------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# GET Top 2 sales from each category\n",
    "\n",
    "partition_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "\n",
    "df_top2 = df.withColumn(\"row_number\", row_number().over(partition_cat))\\\n",
    "            .filter(col(\"row_number\") <= 2)\n",
    "df_top2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5b6a86-4907-4052-ae63-311c23f92292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+-----+----------+\n|  Customer Name|       Category|Sub Category|Sales|row_number|\n+---------------+---------------+------------+-----+----------+\n|Alejandro Grove|      Furniture| Furnishings| 8000|         1|\n|    Joseph Holt|      Furniture|      Tables| 5000|         2|\n|   Ken Lonsdale|Office Supplies|    Supplies| 9000|         1|\n|  Adrian Barton|Office Supplies|     Binders| 3000|         2|\n|    Sean Miller|     Technology|    Machines|22000|         1|\n|   Tamara Chand|     Technology|     Copiers|18000|         2|\n+---------------+---------------+------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "partition_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "\n",
    "df_top2 = df.withColumn(\"row_number\", row_number().over(partition_cat))\n",
    "df_top2 = df_top2.filter(\"row_number <= 2\")\n",
    "df_top2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231fa250-826c-40a4-808b-4a312499091b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+--------+\n|CustermID|CustomerName|      Date|Category|\n+---------+------------+----------+--------+\n|        1|       Alice|2024-01-01|       A|\n|        1|       Alice|2024-01-02|       B|\n|        1|       Alice|2024-01-03|       C|\n|        1|       Alice|2024-01-04|       D|\n|        2|         Bob|2024-02-01|       E|\n|        2|         Bob|2024-02-02|       F|\n|        2|         Bob|2024-02-03|       G|\n|        2|         Bob|2024-02-04|       H|\n|        3|     Charlie|2024-03-01|       I|\n|        3|     Charlie|2024-03-02|       J|\n|        3|     Charlie|2024-03-03|       K|\n|        3|     Charlie|2024-03-04|       L|\n|        4|       David|2024-04-01|       M|\n|        4|       David|2024-04-02|       N|\n|        4|       David|2024-04-03|       O|\n|        4|       David|2024-04-04|       P|\n|        5|        Emma|2024-05-01|       Q|\n|        5|        Emma|2024-05-02|       R|\n|        5|        Emma|2024-05-03|       S|\n|        5|        Emma|2024-05-04|       T|\n+---------+------------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# inirialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveDuplicates\").getOrCreate()\n",
    "\n",
    "# sample data\n",
    "data = [\n",
    "    (1, 'Alice', '2024-01-01', 'A'), (1, 'Alice', '2024-01-02', 'B'), (1, 'Alice', '2024-01-03', 'C'), (1, 'Alice', '2024-01-04', 'D'),\n",
    "    (2, 'Bob', '2024-02-01', 'E'), (2, 'Bob', '2024-02-02', 'F'), (2, 'Bob', '2024-02-03', 'G'), (2, 'Bob', '2024-02-04', 'H'),\n",
    "    (3, 'Charlie', '2024-03-01', 'I'), (3, 'Charlie', '2024-03-02', 'J'), (3, 'Charlie', '2024-03-03', 'K'), (3, 'Charlie', '2024-03-04', 'L'),\n",
    "    (4, 'David', '2024-04-01', 'M'), (4, 'David', '2024-04-02', 'N'), (4, 'David', '2024-04-03', 'O'), (4, 'David', '2024-04-04', 'P'),\n",
    "    (5, 'Emma', '2024-05-01', 'Q'), (5, 'Emma', '2024-05-02', 'R'), (5, 'Emma', '2024-05-03', 'S'), (5, 'Emma', '2024-05-04', 'T')\n",
    "\n",
    "]\n",
    "\n",
    "# create Dataframe\n",
    "\n",
    "df = spark.createDataFrame(data,[\"CustermID\",\"CustomerName\",\"Date\",\"Category\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae03b387-8e07-4c33-9f51-3abaaecbc4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+--------+----------+\n|CustermID|CustomerName|      Date|Category|row_number|\n+---------+------------+----------+--------+----------+\n|        1|       Alice|2024-01-04|       D|         1|\n|        2|         Bob|2024-02-04|       H|         1|\n|        3|     Charlie|2024-03-04|       L|         1|\n|        4|       David|2024-04-04|       P|         1|\n|        5|        Emma|2024-05-04|       T|         1|\n+---------+------------+----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#get latest data transaction for each customer\n",
    "\n",
    "#step1: create a window partition at table level\n",
    "table_partition = Window.partitionBy(\"CustomerName\").orderBy(col(\"Date\").desc())\n",
    "\n",
    "df_ld = df.withColumn(\"row_number\", row_number().over(table_partition))\\\n",
    "          .filter(col(\"row_number\") == 1)\n",
    "df_ld.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d74d777-d323-4b38-8936-0b9902f4cee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+--------+-----+\n|CustermID|CustomerName|      Date|Category|row_n|\n+---------+------------+----------+--------+-----+\n|        1|       Alice|2024-01-04|       D|    1|\n|        2|         Bob|2024-02-04|       H|    1|\n|        3|     Charlie|2024-03-04|       L|    1|\n|        4|       David|2024-04-04|       P|    1|\n|        5|        Emma|2024-05-04|       T|    1|\n+---------+------------+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_ld = df.withColumn(\"row_n\", row_number().over(table_partition))\n",
    "df_ld = df_ld.filter(\"row_n =1\")\n",
    "df_ld.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f33abce-9f22-4bd5-8778-453755519de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|SalesAmt|\n+--------+\n|   10000|\n|   10000|\n|    5000|\n|    5000|\n|    5000|\n|   10000|\n|    3000|\n|    3000|\n|    3000|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# sample list\n",
    "list1 = [\n",
    "    (10000, 'John', '2025-01-01', 'A'),\n",
    "    (10000, 'Jane', '2025-01-02', 'B'),\n",
    "    (5000, 'Alice', '2025-01-03', 'C'),\n",
    "    (5000, 'Bob', '2025-01-04', 'D'),\n",
    "    (5000, 'Charlie', '2025-01-05', 'E'),\n",
    "    (10000, 'David', '2025-01-06', 'F'),\n",
    "    (3000, 'Emma', '2025-01-07', 'G'),\n",
    "    (3000, 'Olivia', '2025-01-08', 'H'),\n",
    "    (3000, 'Sophia', '2025-01-09', 'I')\n",
    "]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(list1, [\"CustermID\", \"CustomerName\", \"Date\", \"Category\"])\n",
    "df = df.select(df.CustermID.alias(\"SalesAmt\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b8138d-4560-452d-aa61-0b344116b72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+----------+\n|SalesAmt| RN|Rank|dense_rank|\n+--------+---+----+----------+\n|   10000|  1|   1|         1|\n|   10000|  2|   1|         1|\n|   10000|  3|   1|         1|\n|    5000|  4|   4|         2|\n|    5000|  5|   4|         2|\n|    5000|  6|   4|         2|\n|    3000|  7|   7|         3|\n|    3000|  8|   7|         3|\n|    3000|  9|   7|         3|\n+--------+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "part_rank = Window.orderBy(col(\"SalesAmt\").desc())\n",
    "df_rank = df.withColumn(\"RN\", row_number().over(part_rank)).withColumn(\"Rank\", rank().over(part_rank)).withColumn(\"dense_rank\", dense_rank().over(part_rank))\n",
    "df_rank.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "WINDOW FUNCTIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}